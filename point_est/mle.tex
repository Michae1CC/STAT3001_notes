\subsection*{Maximum Likelihood Estimates}

\begin{defe}[Maximum Likelihood Estimator] \label{defe: mle}
    For each sample point $\bm{x}$, let $\hat{\theta} (\bm{x})$ be a parameter value at which $L(\theta \mid \bm{x})$ attains its maximum as a function of $\theta$, with $\bm{x}$ held fixed. A {\bf maximum likelihood estimator (MLE)} of the parameter $\theta$ based on a sample $\bm{X}$ is $\hat{\theta} (\bm{X})$ \cite{CasellaGeorge2001SI}*{page 316}.
\end{defe}

\begin{exam}[Normal Likelihood] \label{exam: normal_mle}
    Example taken from \cite{CasellaGeorge2001SI}*{page 316}. Suppose $X_1, \ldots , X_n \iid \Nor (\theta , 1)$, and let $L(\theta \mid \bm{x})$ denote the likelihood function. Then
    \begin{equation*}
        L(\theta \mid \bm{x}) = \prod_{i=1}^{n} \frac{1}{(2 \pi)^{1/2}} \exp \left( -(1/2) (x_i - \theta)^2 \right) = \frac{1}{(2 \pi)^{1/2}} \exp \left( -(1/2) \sum_{i=1}^{n} (x_i - \theta)^2 \right).
    \end{equation*}
    The equation $(d / d\theta) L(\theta \mid \bm{x}) = 0$ reduces to
    \begin{equation*}
        \sum_{i=1}^{n} (x_i - \theta) = 0,
    \end{equation*}
    which has the solution $\hat{\theta} = \overline{x}$. Hence, $\overline{x}$ is a candidate for the MLE. To verify that $\overline{x}$ is, in fact, a global maximim of the likelihood function, we can use the following argument. First, note that $\hat{\theta} = \overline{x}$ is the only solution to $\sum_{i=1}^{n} (x_i - \theta) = 0$; hence $\overline{x}$ is the only zero of the first derivative. Second, verify that
    \begin{equation*}
        \frac{d^2}{d \theta^2} \left. L(\theta \mid \bm{x}) \right|_{\theta = \overline{x}} < 0.
    \end{equation*}
    Thus, $\overline{x}$ is the only extreme point in the interior and it is a maximum. To finally verify that $\overline{x}$ is a global maximum, we must check the boundaries at $\pm \infty$. So $\tilde{\theta} = \overline{x}$ is a global maximum and hence $\overline{X}$ is the MLE.
\end{exam}

\begin{thm} \label{thm: invariance_of_mles}
    If $\hat{\theta}$ is the MLE of $\theta$, the for any function $\tau (\theta)$, the MLE of $\tau (\theta)$ is $\tau (\hat{\theta})$ \cite{CasellaGeorge2001SI}*{page 320}.
\end{thm}

\begin{exam}[Normal MLE, $\mu$ and $\sigma$ unknown] \label{exam: normal_mle_mu_sigma}
    Example taken from \cite{CasellaGeorge2001SI}*{page 321}. Suppose $X_1, \ldots , X_n \iid \Nor (\theta , \sigma^2)$ with both $\mu$ and $\sigma^2$ unknown. Then
    \begin{equation*}
        L(\theta \mid \bm{x}) = \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left( -(1/2) \sum_{i=1}^{n} (x_i - \theta)^2 / \sigma^2 \right)
    \end{equation*}
    and
    \begin{equation*}
        \ln L(\theta \mid \bm{x}) = -\frac{n}{2} \ln 2 \pi - \frac{n}{2} \ln \sigma^2 - \frac{1}{2} \sum_{i=1}^{n} (x_i - \theta)^2 / \sigma^2 .
    \end{equation*}
    The partial derivatives, with respect to $\theta$ and $\sigma^2$ are
    \begin{equation*}
        \frac{\partial}{\partial \theta} \ln L(\theta \mid \bm{x}) = \frac{1}{\sigma^2} \sum_{i=1}^{n} (x_i - \theta)
    \end{equation*}
    and
    \begin{equation*}
        \frac{\partial}{\partial \sigma^2} \ln L(\sigma^2 \mid \bm{x}) = - \frac{1}{2\sigma^2} + \frac{1}{2 \sigma^4} \sum_{i=1}^{n} (x_i - \theta).
    \end{equation*}
    Setting the partial derivatives equal to $0$ and solving for the solution $\hat{\theta} = \overline{x}, \; \hat{\sigma}^2 = n^{-1} \sum_{i=1}^{n} (x_i - \overline{x})$. To verify that this solution is, in fact, a global maximum, recall first that if $\theta \neq \overline{x}$, then $\sum (x_i - \theta)^2 > \sum (x_i - \overline{x})^2$. Hence, for any value of $\sigma^2$,
    \begin{equation*}
        \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left( -(1/2) \sum_{i=1}^{n} (x_i - \overline{x})^2 / \sigma^2 \right) \geq \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left( -(1/2) \sum_{i=1}^{n} (x_i - \theta)^2 / \sigma^2 \right) .
    \end{equation*}
    Therefore, verifying that we have found the maximum likelihood estimators is reduced to a one-dimensional problem, verifying that $(\sigma^2)^{-n/2} \exp \left( -\frac{1}{2} \sum (x_i - \overline{x})^2 / \sigma^2 \right)$ achieves its global maximum at $\sigma^2 = n^{-1} \sum (x_i - \overline{x})^2$. This is straightforward to do using univariate calculus and, in fact, the estimators $\left( \overline{X} , n^{-1} \sum \left( X_i - \overline{X} \right)^2 \right)$ are the MLEs.
\end{exam}

\begin{defe}[Quantile of Order $\alpha$] \label{defe: quant_ord}
    The quantile of order $\alpha$, $q_{\alpha}$, is the value of the random variable $X$ such that
    \begin{equation*}
        \PP \left[ X \leq q_{\alpha} \right] = \alpha.
    \end{equation*}
\end{defe}

\begin{exam}
    Let $X_1 , \ldots , X_n$ denote a random sample from a $N (\mu , \sigma^2)$ distribution, where both $\mu$ and $\sigma$ are unknown. Let us consider a way to find the maximum likelihood for quantile of order $\alpha$. Take $X \sim N \left( \mu , \sigma^2 \right)$. As $Z = (X  - \mu) / \sigma$ has a standard normal distribution with the function $\Phi (z)$, we can express the left hand for the expression of the quantile of order $\alpha$ as
    \begin{equation*}
        \PP \left[ X \leq q_{\alpha} \right] = \PP \left[ \frac{X - \mu}{\sigma} \leq \frac{q_{\alpha} - \mu}{\sigma} \right] = \PP \left[ Z \leq \frac{q_{\alpha} - \mu}{\sigma} \right].
    \end{equation*}
    This means
    \begin{align*}
        \frac{q_{\alpha} - \mu}{\sigma} & = \Phi^{-1} (\alpha)               \\
        q_{\alpha}                      & = \mu + \sigma \Phi^{-1} (\alpha).
    \end{align*}
    Hence, as a consequence of \Cref{thm: invariance_of_mles}, $g(\hat{\bm{\theta}}) = \hat{\mu} + \hat{\sigma} \Phi^{-1} (\alpha)$ is the maximum likelihood estimate of $q_{\alpha}$. Note, however, that this is not an unbiased estimate of $q_{\alpha}$ by virtue of the fact that $\EE [\hat{\sigma}] \neq \sigma$. If we adjusted this estimate by multiplying by some constant $k_n$, that is,
    \begin{equation*}
        \EE [\hat{\sigma}] = k_n \sigma
    \end{equation*}
    then it would be an unbiased estimator of $q_{\alpha}$. To compute such a $k_n$, we have that $n \hat{\sigma}^2 / \sigma^2 \sim \chi_{n-1}^2$ that is, $\frac{1}{2} n \hat{\sigma}^2 / \sigma^2 \sim \gamma (m/2)$ where $m = (n-1)/2$. This is equivalent to saying $\hat{\sigma} = \sigma \sqrt{2/n} \sqrt{Y}$ where $Y \sim \gamma (m)$. Thus $\EE [\hat{\sigma}] = k_n \sigma$, where $k_n = \sqrt{2/n} \EE [\sqrt{Y}]$ and where
    \begin{align*}
        \EE [\sqrt{Y}] \
         & = \frac{\int_{0}^{\infty} y^{1/2} \exp (-y) y^{m-1} \; dy}{\Gamma (m)}     \\
         & = \frac{\int_{0}^{\infty} \exp (-y) y^{m+\frac{1}{2}-1} \; dy}{\Gamma (m)} \\
         &= \frac{\Gamma (m + \frac{1}{2})}{\Gamma(m)} \\
         &= \frac{\Gamma (\frac{n}{2})}{\Gamma(\frac{n-1}{2})}
    \end{align*}
    Upon substituting the result for $\EE [\sqrt{Y}]$ into the right-hand side of of expression for $k_n$ we obtain
    \begin{equation*}
        k_n = \sqrt{\frac{2}{n}} \frac{\Gamma (\frac{n}{2})}{\Gamma(\frac{n-1}{2})} .
    \end{equation*}
\end{exam}