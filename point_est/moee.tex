\subsection*{Methods of Evaluating Estimators}

\begin{defe}[Mean Square Error] \label{defe: mse}
    The {\bf mean square error} (MSE) of an estimator $W$ of a parameter $\theta$ is the function $\theta$ defined by $\EE_{\theta} (W - \theta)^2$ \cite{CasellaGeorge2001SI}*{page 330}.
\end{defe}

\begin{defe}[Bias] \label{defe: bias}
    The {\bf bias} of an estimator $W$ of a parameter $\theta$ is the difference between the expected value of $W$ and $\theta$; that is $\Bias_{\theta} W = \EE_{\theta} W - \theta$. An estimator whose bias is identically (in $\theta$) equal to $0$ is called an {\bf unbiased estimator} and satisfies $\EE_{\theta} W = \theta$ for all $\theta$ \cite{CasellaGeorge2001SI}*{page 330}.
\end{defe}

It is important to note that
\begin{equation*}
    \EE_{\theta} \left( W - \theta \right)^{2} = \Var_{\theta} + \left( \EE_{\theta} W - \theta \right)^2 = \Var_{\theta} W + \left( \Bias_{\theta} W \right)^2 .
\end{equation*}

\begin{exam}[Normal MSE] \label{exam: norm_mse_p1}
    Example taken from \cite{CasellaGeorge2001SI}*{page 331}. Let $X_1 , \ldots , X_n \iid \Nor (\mu , \sigma^2)$. The statistics $\overline{X}$ and $S^2$ are both unbiased estimators since
    \begin{equation*}
        \EE \overline{X} = \mu, \quad \EE S^2 = \sigma^2, \; \text{for all} \; \mu \; \text{and} \; \sigma^2.
    \end{equation*}
    The MSEs of these estimators are given by
    \begin{align*}
        \EE \left( \overline{X} - \mu \right)^2 & = \Var \overline{X} = \frac{\sigma^2}{n} \\
        \EE \left( S^2 - \sigma^2 \right)^2     & = \Var S^2 = \frac{2 \sigma^4}{n - 1}.
    \end{align*}
    The MSE of $\overline{X}$ remains $\sigma^2 / n$ even if the normality assumption is dropped. However, the above expression for the MSE of $S^2$ does not remain the same if the normality assumption is relaxed. An alternative estimator for $\sigma^2$ is the MLE $\hat{\sigma} = \frac{1}{n} \sum_{i=1}^{n} \left( X_i - \overline{X} \right)^2 = \frac{n-1}{n} S^2$. It is straightforward to calculate
    \begin{equation*}
        \EE \hat{\sigma}^2 = \EE \left( \frac{n-1}{n} S^2 \right) = \frac{n-1}{n} \sigma^2,
    \end{equation*}
    so that $\hat{\sigma}^2$ is a biased estimator of $\sigma^2$. The variance of $\hat{\sigma}^2$ can also be calculated as
    \begin{equation*}
        \Var \; \hat{\sigma}^2 = \Var \left( \frac{n-1}{n} S^2 \right) = \left( \frac{n-1}{n} \right)^2 \Var S^2 = \frac{2(n-1) \sigma^4}{n^2},
    \end{equation*}
    and hence, its MSE is given by
    \begin{equation*}
        \EE \left( \hat{\sigma}^2 - \sigma^2 \right) = \frac{2(n-1)\sigma^4}{n^2} + \left( \frac{n-1}{n} \sigma^2 - \sigma^2 \right)^2 = \left( \frac{2n-1}{n^2} \right) \sigma^4.
    \end{equation*}
    Thus we have
    \begin{equation*}
        \EE \left( \hat{\sigma}^2 - \sigma^2 \right)^2 = \left( \frac{2n-1}{n^2} \right) \sigma^4  < \left( \frac{2}{n-1} \right) \sigma^4 = \EE \left( \hat{\sigma}^2 - \sigma^2 \right)^2,
    \end{equation*}
    showing that $\hat{\sigma}^2$ has a smaller MSE than $S^2$. Thus, by trading off variance for bias, the MSE is improved.
\end{exam}

\begin{defe}[Best Unbiased Estimator] \label{defe: umvu}
    An estimator $W^{\ast}$ is a {\bf best unbiased estimator} of $\tau (\theta)$ if it satisfies $EE_{\theta} W^{\ast} = \tau (\theta)$ for all $\theta$ and, for any other estimator $W$ with $\EE_{\theta} W = \tau (\theta)$, we have $\Var_{\theta} W^{\ast} \leq \Var_{\theta} W$ for all $\theta$. $W^{\ast}$ is also called a {\bf uniform minimum variance unbiased estimator} (UMVUE) of $\tau (\theta)$ \cite{CasellaGeorge2001SI}*{page 334}.
\end{defe}

\begin{thm}[Cramer-Rao Inequality] \label{thm: cri_neq}
    Let $X_1 , \ldots , X_n$ be a sample with pdf $f(\bm{x} \mid \theta)$, and let $W(\bm{X}) = W(X_1 , \ldots , X_n)$ be any estimator satisfying
    \begin{equation*}
        \frac{d}{d \theta} \EE_{\theta} W(\bm{X}) = \int_{\calX} \frac{\partial}{\partial \theta} \left[ W(\bm{x}) f(\bm{x} \mid \theta) \right]
    \end{equation*}
    and
    \begin{equation*}
        \Var_{\theta} W(\bm{X}) < \infty.
    \end{equation*}
    Then
    \begin{equation*}
        \Var_{\theta} (W(\bm{X})) \geq \frac{\left( \frac{d}{d \theta} \EE_{\theta} W(\bm{X}) \right)^2}{\EE_{\theta} \left( \left( \frac{\partial}{\partial \theta} \ln f(\bm{X} \mid \theta)  \right)^2 \right)} = \frac{\left( \frac{d}{d \theta} \EE_{\theta} W(\bm{X}) \right)^2}{\calJ (\theta)}
    \end{equation*}
    which is commonly refereed to as the {\bf minimum variance bound} (MVB). If $W(\bm{X})$ attains the MVB (for all values of $\theta$), it is said to be a MVB estimator \cite{CasellaGeorge2001SI}*{page 335}.
\end{thm}

\begin{cor}[Cramer-Rao Inequality, iid Case] \label{cor: cri_neq_iid}
    If the assumptions of \Cref{thm: cri_neq} are satisfied and, additionally, if $X_1 , \ldots , X_n$ are iid with pdf $f(\bm{x} \mid \theta)$, then
    \begin{equation*}
        \Var_{\theta} (W(\bm{X})) \geq \frac{\left( \frac{d}{d \theta} \EE_{\theta} W(\bm{X}) \right)^2}{n \EE_{\theta} \left( \left( \frac{\partial}{\partial \theta} \ln f(X \mid \theta)  \right)^2 \right)}
    \end{equation*}
    \cite{CasellaGeorge2001SI}*{page 337}.
\end{cor}

\begin{lem} \label{lem: cri_helper}
    If $f(\bm{x} \mid \theta)$ satisfies
    \begin{equation*}
        \frac{d}{d \theta} \EE_{\theta} \left( \frac{\partial}{\partial \theta} \ln f \left( X \mid \theta \right) \right) = \int \frac{\partial}{\partial \theta} \left[ \left( \frac{\partial}{\partial \theta} \ln f (x \mid \theta) \right) f (x \mid \theta) \right] \; dx
    \end{equation*}
    (true for the exponential family), then
    \begin{equation*}
        \EE_{\theta} \left( \left( \frac{\partial}{\partial \theta} \ln f(X \mid \theta)  \right)^2 \right) = - \EE_{\theta} \left( \frac{\partial^2}{\partial \theta^2} \ln f(X \mid \theta) \right)
    \end{equation*}
    \cite{CasellaGeorge2001SI}*{page 338}.
\end{lem}

\begin{exam}[Poisson Unbiased Estimate] \label{exam: poi_unbiased_est}
    Example taken from \cite{CasellaGeorge2001SI}*{page 338}. Let $X_1 , \ldots , X_n \iid \Poi (\lambda)$, and let $\overline{X}$ and $S^2$ be the sample mean and variance, respectively. Recall that for the Poisson pmf both the mean and variance are equal to $\lambda$. We have
    \begin{align*}
        \EE_{\lambda} \overline{X} & = \lambda, \quad \text{for all} \; \lambda, \\
        \EE_{\lambda} S^2          & = \lambda, \quad \text{for all} \; \lambda,
    \end{align*}
    so both $\overline{X}$ and $S^2$ are unbiased estimators of $\lambda$. To determine the better estimator, $\overline{X}$ or $S^2$, we should now compare the variances. We have $\Var_{\lambda} \overline{X} = \lambda / n$, but $\Var_{\lambda} S^2$ is quiet a lengthy calculation. Not only this, even if we can establish that $\overline{X}$ is better than $S^2$, consider the class of estimators
    \begin{equation*}
        W_a \left( \overline{X} , S^2 \right) = a \overline{X} + (1-a) S^2.
    \end{equation*}
    For every constant $a$, $\EE_{\lambda} W_a = \lambda$, so now we have infinitely many unbiased estimators of $\lambda$. Instead, let us show that $\overline{X}$ is the best estimator directly using the Cramer-Rao inequality. Here we are estimating $\tau(\lambda) = \lambda$, so that $\tau ' (\lambda) = 1$. Also, since we have an exponential family, using \Cref{lem: cri_helper} gives us
    \begin{align*}
        \EE_{\lambda} \left( \left( \frac{\partial}{\partial \lambda} \ln f(X \mid \lambda)  \right)^2 \right) \
         & = - n \EE_{\lambda} \left( \frac{\partial^2}{\partial \lambda^2} \ln f(X \mid \lambda) \right)                                \\
         & = - n \EE_{\lambda} \left( \frac{\partial^2}{\partial \lambda^2} \ln \left( \frac{e^{-\lambda} \lambda^X}{X!} \right) \right) \\
         & = - n \EE_{\lambda} \left( \frac{\partial^2}{\partial \lambda^2} \left( -\lambda + X \ln \lambda - \ln X! \right) \right)     \\
         & = - n \EE_{\lambda} \left( - \frac{X}{\lambda^2} \right)                                                                      \\
         & = \frac{n}{\lambda} .
    \end{align*}
    Hence for any unbiased estimator, $W$, of $\lambda$, from \Cref{cor: cri_neq_iid} we must have
    \begin{align*}
        \Var_{\theta} (W(\bm{X})) \
         & \geq \frac{\left( \frac{d}{d \theta} \EE_{\theta} W(\bm{X}) \right)^2}{n \EE_{\theta} \left( \left( \frac{\partial}{\partial \theta} \ln f(X \mid \theta)  \right)^2 \right)} \\
         & = \frac{\left( 1 \right)^2}{\left( \frac{n}{\lambda} \right)}                                                                                                                 \\
         & = \frac{\lambda}{n}.
    \end{align*}
    Since $\Var_{\lambda} \overline{X} = \lambda / n$, $\overline{X}$ must be the best unbiased estimator.
\end{exam}

\begin{cor}[Attainment] \label{cor: cri_attainment}
    Let $X_1 , \ldots , X_n$ be a sample with pdf $f(\bm{x} \mid \theta)$, where $f(x \mid \theta)$ satisfies the conditions of the Cramer-Rao Theorem. $L(\theta \mid \bm{x}) = \prod_{i=1}^{n} f(x_1 \mid \theta)$ denote the likelihood function. If $W(\bm{X}) = W(X_1 , \ldots , X_n)$ is any unbiased estimator of $\tau (\theta)$, then $W(\bm{X})$ attains the Cramer-Rao Lower Bound if and only if
    \begin{equation*}
        a(\theta) \left[ W(\bm{x}) - \tau (\theta) \right] = \frac{\partial}{\partial \theta} \ln L (\theta \mid \bm{x})
    \end{equation*}
    for some function $a(\theta)$ \cite{CasellaGeorge2001SI}*{page 341}.
\end{cor}

\begin{exam} \label{exam: MSE_Cramer_ineq}
    Example taken from Tutorial Sheet 2 Q5. Let $T$ be an estimator of the parameter $\theta$, having bias $b(\theta)$. Assuming that the usual regularity conditions and using the Cramer-Rao lower bound (\Cref{thm: cri_neq}) for the variance of an unbiased estimator of $\theta$, we can show that
    \begin{equation*}
        \MSE (T) \geq \left[ 1 + \frac{\partial}{\partial \theta} b(\theta) \right]^2 \cdot \calJ^{-1} (\theta) + [b(\theta)]^2
    \end{equation*}
    where $\calJ (\theta)$ is the Fisher information matrix (\Cref{defe: fim}). To start, since $b(\theta) = \EE [\theta] - \theta$ we have
    \begin{equation*}
        \EE [T] = \theta + b(\theta) \triangleq g(\theta)
    \end{equation*}
    so that $T$ is an unbiased estimate for $g(\theta)$. By the Cramer-Rao lower bound,
    \begin{equation*}
        \Var (T) \geq [g'(\theta)]^2 \cdot \calJ^{-1} (\theta) = \left[ 1 + \frac{\partial}{\partial \theta} b(\theta) \right]^2 \cdot \calJ^{-1} (\theta).
    \end{equation*}
    Now
    \begin{align*}
        \MSE (T) \
         & = \Var (T) + [\Bias (T)]^2                                                                                      \\
         & = \Var (T) + [b(\theta)]^2                                                                                      \\
         & \geq \left[ 1 + \frac{\partial}{\partial \theta} b(\theta) \right]^2 \cdot \calJ^{-1} (\theta) + [b(\theta)]^2.
    \end{align*}
\end{exam}

\begin{exam}[Continuation of \Cref{exam: normal_mle_mu_sigma}] \label{exam: norm_mse_p2}
    Example taken from \cite{CasellaGeorge2001SI}*{page 341}. Here we know
    \begin{equation*}
        L(\mu , \sigma^2 \mid \bm{x}) = \frac{1}{(2 \pi \sigma^2)^{n/2}} \exp \left( -(1/2) \sum_{i=1}^{n} (x_i - \mu)^2 / \sigma^2 \right),
    \end{equation*}
    and hence
    \begin{equation*}
        \frac{\partial}{\partial \sigma^2} \ln L(\mu , \sigma^2 \mid \bm{x}) = \frac{n}{2 \sigma^4} \left( \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{n} - \sigma^2 \right).
    \end{equation*}
    Thus, taking $a(\sigma^2) = n / (2 \sigma^4)$ shows that the best unbiased estimator of $\sigma^2$ is $\frac{(x_i - \mu)^2}{n}$, which is calculable only if $\mu$ is known. If $\mu$ is not known, the bound {\it cannot} be attained.
\end{exam}

\subsubsection*{Sufficiency and Unbiasedness}

\begin{thm}[Rao-Blackwell] \label{thm: rao_bw_thrm}
    Let $W$ be any unbiased estimator of $\tau (\theta)$, and let $T$ be a sufficient statistic for $\theta$. Define $\phi (T) = \EE (W \mid T)$. Then $\EE_{\theta} \phi (T) = \tau (\theta)$ and $\Var_{\theta} \phi (T) \leq \Var_{\theta} W$ for all $\theta$; that is, $\phi (T)$ is a uniformly better unbiased estimator of $\tau (\theta)$ \cite{CasellaGeorge2001SI}*{page 342}.
\end{thm}

\begin{thm} \label{thm: best_unbiased_uniq}
    If $W$ is the best unbiased estimator of $\tau (\theta)$, then $W$ is unique \cite{CasellaGeorge2001SI}*{page 343}.
\end{thm}

\begin{thm} \label{thm: best_css_est}
    Let $T$ be a complete sufficient statistic for a parameter $\theta$, and let $\phi (T)$ be any estimator based only on $T$. Then $\phi (T)$ is the best unbiased estimator of its expected value \cite{CasellaGeorge2001SI}*{page 347}.
\end{thm}

\subsubsection*{Consistency}

\begin{defe}[Consistency] \label{defe: consistent}
    A sequence of estimators $T_n$ of $g(\bm{\theta})$ is said to be {\bf consistent} if for every $\bm{\theta} \in \Omega$,
    \begin{equation*}
        T_n \overset{\PP_{\bm{\theta}}}{\to} g (\bm{\theta}), \quad \text{as} \; n \to \infty
    \end{equation*}
    that is, given any $\varepsilon > 0$, then
    \begin{equation*}
        \PP \left[ \left| T_n \left( \bm{X}_1 , \ldots , \bm{X}_n \right) - g (\bm{\theta}) \right| \geq \varepsilon \right] \to 0, \quad \text{as} \; n \to \infty
    \end{equation*}
    \cite{KroeseDirkP2013SMaC}*{page 176} [Background Notes, page 44].
\end{defe}

\begin{thm} \label{thm: var_bias_consistent}
    If $\Var (T_n) \to 0$ and $\Bias (T_n) \to 0$, as $n \to \infty$, then the sequence of estimates $T_n$ is consistent for estimating $g(\bm{\theta})$ [Background Notes, page 44].
\end{thm}

\begin{proof}
    Let $f(\bm{x}_1 , \ldots , \bm{x}_n ; \bm{\theta})$ denote the joint pdf of $\bm{X}_1 , \ldots , \bm{X}_n$. Then we have
    \begin{equation*}
        \PP \left[ \left| T_n - g (\bm{\theta}) \right| \geq \varepsilon \right] = {\int \ldots \int}_{\left| T_n - g (\bm{\theta}) \right| \geq \varepsilon} f(\bm{x}_1 , \ldots , \bm{x}_n ; \bm{\theta}) \; d \bm{x}_1 \; \ldots \; d \bm{x}_n.
    \end{equation*}
    On the region of integration in the above,
    \begin{align*}
        \left| T_n - g (\bm{\theta}) \right|                         & \geq \varepsilon   \\
        \left( T_n - g (\bm{\theta}) \right)^2                       & \geq \varepsilon^2 \\
        \frac{\left( T_n - g (\bm{\theta}) \right)^2}{\varepsilon^2} & \geq 1,
    \end{align*}
    and since $f(\bm{x}_1 , \ldots , \bm{x}_n ; \bm{\theta})$ is non-negative,
    \begin{equation*}
        f(\bm{x}_1 , \ldots , \bm{x}_n ; \bm{\theta}) \leq f(\bm{x}_1 , \ldots , \bm{x}_n ; \bm{\theta}) \frac{\left( T_n - g (\bm{\theta}) \right)^2}{\varepsilon^2}.
    \end{equation*}
    Thus
    \begin{align*}
         & \PP \left[ \left| T_n - g (\bm{\theta}) \right| \geq \varepsilon \right]                                                                                                                                                         \\
         & = {\int \ldots \int}_{\left| T_n - g (\bm{\theta}) \right| \geq \varepsilon} f(\bm{x}_1 , \ldots , \bm{x}_n ; \bm{\theta}) \; d \bm{x}_1 \; \ldots \; d \bm{x}_n                                                                 \\
         & \leq {\int \ldots \int}_{\left| T_n - g (\bm{\theta}) \right| \geq \varepsilon} f(\bm{x}_1 , \ldots , \bm{x}_n ; \bm{\theta}) \frac{\left( T_n - g (\bm{\theta}) \right)^2}{\varepsilon^2} \; d \bm{x}_1 \; \ldots \; d \bm{x}_n \\
         & \leq \frac{1}{\varepsilon^2} {\int \ldots \int} f(\bm{x}_1 , \ldots , \bm{x}_n ; \bm{\theta}) \left( T_n - g (\bm{\theta}) \right)^2 \; d \bm{x}_1 \; \ldots \; d \bm{x}_n                                                       \\
         & = \frac{1}{\varepsilon^2} \MSE (T_n)                                                                                                                                                                                             \\
         & = \frac{1}{\varepsilon^2} \left[ \Var (T_n) + \left( \Bias (T_n) \right)^2 \right]
    \end{align*}
    which tends to $0$ as $n \to \infty$, since $\Var (T_n)$ and $\Bias (T_n)$ both tend to $0$.
\end{proof}

\begin{exam}[Continuation of \Cref{exam: norm_mse_p1}] \label{exam: exam: norm_consistent_var}
    Example taken from Background Notes, page 44. The estimators
    \begin{equation*}
        s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2 , \quad \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \overline{x})^2
    \end{equation*}
    are both consistent for $\sigma^2$. This is easily seen with $s^2$ since
    \begin{equation*}
        \EE [s^2] = \sigma \quad \text{and} \quad \Var (s^2) \to 0
    \end{equation*}
    as $n \to \infty$. To show that $\hat{\sigma}^2$ is also a consistent estimator, note that
    \begin{equation*}
        \frac{\sum_{j=1}^{n} \left(X_j - \overline{X} \right)^2}{\sigma^2} \sim \chi_{n-1}^{2} \tag{see \ref{eqn: norm_chi_sq}}
    \end{equation*}
    meaning
    \begin{equation*}
        \EE \left[ \frac{\sum_{j=1}^{n} \left(X_j - \overline{X} \right)^2}{\sigma^2} \right] =  n-1
    \end{equation*}
    and
    \begin{equation*}
        \Var \left[ \frac{\sum_{j=1}^{n} \left(X_j - \overline{X} \right)^2}{\sigma^2} \right] =  2(n-1)
    \end{equation*}
    so that
    \begin{equation*}
        \EE \left[ \sum_{j=1}^{n} \left(X_j - \overline{X} \right)^2 \right] =  (n-1) \sigma^2
    \end{equation*}
    and
    \begin{equation*}
        \Var \left[ \sum_{j=1}^{n} \left(X_j - \overline{X} \right)^2 \right] =  2(n-1) \sigma^4 .
    \end{equation*}
    Hence
    \begin{align*}
        \EE \left( \hat{\sigma}^2 \right)  & = \frac{n-1}{n} \sigma^2 = \sigma^2 - \frac{\sigma^2}{n} \\
        \Var \left( \hat{\sigma}^2 \right) & = \frac{2 (n-1) \sigma^4}{n^2}
    \end{align*}
    meaning $\EE \left( \hat{\sigma}^2 \right) \to 0$ and $\Var \left( \hat{\sigma}^2 \right) \to 0$ as $n \to \infty$. Therefore, by \Cref{thm: var_bias_consistent}, $\hat{\sigma}^2$ is a consistent estimator of $\sigma^2$.
\end{exam}

\begin{exam}[Conclusion of \Cref{exam: uni_ss_p1}] \label{exam: uni_ss_p2}
    Example taken from \cite{CasellaGeorge2001SI}*{page 338} and can also be found on tutorial sheet 3. We saw from \Cref{exam: uni_ss_p1} that the likelihood was
    \begin{align*}
        L (\theta) & = \theta^{-n} \Id_{N_\theta} (x_{(n)}) \left( \prod_{i=1}^{n} \Id_{N} (x_i) \right)             \\
                   & = \theta^{-n} \Id_{[0,\theta]} (x_{(n)}) \left( \prod_{i=1}^{n} \Id_{[0,\infty)} (x_i) \right).
    \end{align*}
    Thus $L (\theta) = \theta^{-n}$ when $\theta \geq x_{(n)}$ and $0$ otherwise. This implies the MLE of $\theta$ is $\theta_{(n)}$. Since $\frac{\partial}{\partial \theta} \ln f (x \mid \theta) = -\frac{1}{\theta}$, we have
    \begin{equation*}
        \EE_{\theta} \left( \left( \frac{\partial}{\partial \theta} \ln f (x \mid \theta) \right)^2 \right) = \frac{1}{\theta^2}.
    \end{equation*}
    The Cramer-Rao Theorem (see \Cref{thm: cri_neq}) suggests that if $W$ is any unbiased estimator of $\theta$ then,
    \begin{equation*}
        \VV_\theta W \geq \frac{\theta^2}{n}.
    \end{equation*}
    We would like to find an unbiased estimator with small variance. As a first guess, consider the sufficient statistic $Y = X_{(n)}$, the largest order statistic and MLE. The pdf of $Y$ is $f_Y (y\mid \theta) = ny^{n-1} / \theta^n, \; 0 < y < \theta$, so
    \begin{equation*}
        \EE_{\theta} Y = \int_{0}^{\theta} \frac{ny^{n-1}}{\theta^n} \; dy = \frac{n}{n+1} \theta,
    \end{equation*}
    showing that $\frac{n+1}{n}Y$ is an unbiased estimator of $\theta$. We next calculate
    \begin{align*}
        \VV_{\theta} \left( \frac{n+1}{n} Y \right) \ 
        & = \left( \frac{n+1}{n} \right)^2 \VV_{\theta} (Y)                                                             \\
        & = \left( \frac{n+1}{n} \right)^2 \left( \EE_{\theta} Y^2 - \left( \frac{n}{n+1} \theta \right)^2 \right)      \\
        & = \left( \frac{n+1}{n} \right)^2 \left( \frac{n}{n+2}\theta^2 - \left( \frac{n}{n+1} \theta \right)^2 \right) \\
        & = \frac{1}{n(n+2)} \theta^2,
    \end{align*}
    which is uniformly smaller than $\frac{\theta^2}{n}$. This indicates that the Cramer-Rao Theorem is not applicable to this pdf. To see this is so, we can use the Leibnitz's Rule which states that if $f(x , \theta), a(\theta)$ and $b(\theta)$ are differentiable with respect to $\theta$, then
    \begin{equation*}
        \frac{d}{d \theta} \int_{a(\theta)}^{b(\theta)} f(x, \theta) d x=f(b(\theta), \theta) \frac{d}{d \theta} b(\theta)-f(a(\theta), \theta) \frac{d}{d \theta} a(\theta)+\int_{a(\theta)}^{b(\theta)} \frac{\partial}{\partial \theta} f(x, \theta) d x
    \end{equation*}
    we can calculate
    \begin{align*}
        \frac{d}{d \theta} \int_{0}^{\theta} h(x) f(x \mid \theta) \; d x \
         & =\frac{d}{d \theta} \int_{0}^{\theta} h(x) \frac{1}{\theta} \; d x                                                     \\
         & = \frac{h(\theta)}{\theta}+\int_{0}^{\theta} h(x) \frac{\partial}{\partial \theta}\left(\frac{1}{\theta}\right) \; d x \\
         & \neq \int_{0}^{\theta} h(x) \frac{\partial}{\partial \theta} f(x \mid \theta) \; d x,
    \end{align*}
    unless $h(\theta) / \theta = 0$ for all $\theta$. Hence the Cramer-Rao Theorem does not apply. In general, if the range of the pdf depends on the parameter, the theorem will no be applicable.
\end{exam}

\subsubsection*{Large-Sample Comparisons of Estimators}

\begin{thm}[Asymptotic Distribution of the MLE] \label{thm: large_samp_est}
    Suppose that $\hat{\bm{\theta}}_n$ is a sequence of consistent ML estimates for $\bm{\theta}$. Then $\sqrt{n} \left( \hat{\bm{\theta}}_n - \bm{\theta} \right)$ converges in distribution to a $\Nor \left( \bm{0} , \mathring{\calJ}^{-1} (\bm{\theta}) \right)$ distributed random vector as $n \to \infty$. In other words,
    \begin{align*}
        \hat{\bm{\theta}}_n \overset{\text{approx}}{\sim} \Nor \left( \bm{\theta} , \mathring{\calJ}^{-1} (\bm{\theta}) / n \right) .
    \end{align*}
\end{thm}

\begin{thm}[Multivariate Central Limit Theorem] \label{thm: mv_cen_lim_thrm}
    Let $\bm{X}_1 , \bm{X}_2 , \ldots , \bm{X}_n \iid \Wn (\bm{\mu} , \bm{\Sigma})$. For large $n$, the random vector $\sum_{i} \bm{X}_i$ approximately has a $\Nor \left( n \bm{\mu} , \bm{\Sigma} \right)$ distribution.
\end{thm}

\begin{thm}[Delta Method] \label{thm: delta_ethod}
    Let $\bm{Z}_1 , \bm{Z}_2 , \ldots $ be a sequence of random vectors such that $\sqrt{n} \left( \bm{Z}_{n} - \bm{\mu} \right) \to \bm{K} \sim \Nor (\bm{0} , \bm{\Sigma})$ as $n \to \infty$. Then for any continuously differentiable function $g$ of $\bm{Z}_{n}$,
    \begin{equation*}
        \sqrt{n} \left( g (\bm{Z}_n) - g (\bm{\mu}) \right) \to \bm{R} \sim \Nor \left( \bm{0} , \bm{J} \bm{\Sigma} \bm{J}^{\intercal} \right),
    \end{equation*}
    where $\bm{J} = \bm{J} (\bm{\mu}) = \left( \frac{\partial g_i (\bm{\mu})}{\partial x_j} \right)$ is the Jacobian of the matrix $g$ evaluated at $\bm{\mu}$ \cite{KroeseDirkP2013SMaC}*{page 92}.
\end{thm}

\begin{defe}[Asymptotic Relative Efficiency] \label{defe: are}
    Suppose that $\hat{\theta}_{n_1}$ and $\hat{\theta}_{n_2}$ are two single variable estimates such that
    \begin{align*}
        \hat{\theta}_{n_1} & \overset{\text{approx}}{\sim} \Nor \left( \theta , \tau_1^2 / n \right)  \\
        \hat{\theta}_{n_2} & \overset{\text{approx}}{\sim} \Nor \left( \theta , \tau_2^2 / n \right).
    \end{align*}
    The {\bf Asymptotic Relative Efficiency} (ARE) of $\hat{\theta}_{n_2}$ with respect to $\hat{\theta}_{n_1}$ is given by
    \begin{equation*}
        \operatorname{ARE} (\hat{\theta}_{n_2}) = \tau_1^2 / \tau_2^2
    \end{equation*}
    [Background Notes, page 46].
\end{defe}

\begin{exam}[Asymptotic Distribution of Bernoulli MLE] \label{exam: asym_dist_ber}
    Example taken from \cite{KroeseDirkP2013SMaC}*{page 177}. For $X_1 , \ldots , X_n \iid \Ber (p)$, the MLE for $p$ is
    \begin{equation*}
        \hat{p}_n = \overline{x} = \frac{1}{n} \sum_i x_i .
    \end{equation*}
    To compute the information number (see \Cref{defe: fim}) for $p$, note that regularity conditions hold so that
    \begin{equation*}
        \mathring{\calJ} (p) = \Var_{p} \left( S (p) \right)
    \end{equation*}
    where
    \begin{align*}
        S (p) & = \frac{d}{dp} \ln \left( p^x (1-p)^{1-x} \right)               \\
              & = \frac{d}{dp} \left[ x \cdot \ln (p) + (1-x) \ln (1-p) \right] \\
              & = \frac{x}{p} - \frac{(1-x)}{1-p} = \frac{x - \theta}{p(1-p)}.
    \end{align*}
    This means that
    \begin{align*}
        \mathring{\calJ} (p) & = \Var_{p} \left( S (p) \right)                  \\
                             & = \Var_p \left( \frac{X- \theta}{p(1-p)} \right) \\
                             & = \Var_p \left( \frac{X}{p(1-p)} \right)         \\
                             & = \frac{p(1-p)}{p^2 (1-p)^2} = \frac{1}{p(1-p)}.
    \end{align*}
    \Cref{thm: large_samp_est} states that
    \begin{equation*}
        \hat{p}_n \overset{\text{approx}}{\sim} \Nor \left( p , \frac{p(1-p)}{n} \right) .
    \end{equation*}
\end{exam}